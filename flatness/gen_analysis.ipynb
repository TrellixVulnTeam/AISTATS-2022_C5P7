{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo = \"https://raw.githubusercontent.com/nicoguaro/matplotlib_styles/master\"\n",
    "# style.use(\"results/style_sheet.mplstyle\")\n",
    "\n",
    "rc('figure', figsize=(8, 4))\n",
    "rc('savefig', bbox='tight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import resnet18_narrow as resnet18\n",
    "from utils import get_loader\n",
    "from utils.train_utils import AverageMeter, accuracy\n",
    "import argparse\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "#               'ms': [0],\n",
    "              'mo': [0.0, 0.5, 0.9],  # momentum\n",
    "              'width': [4, 6, 8],  # network width\n",
    "              'wd': [0.0, 1e-4, 5e-4],  # weight decay\n",
    "              'lr': [0.01, 0.0075, 0.005],  # learning rate\n",
    "              'bs': [32, 128, 512],  # batch size\n",
    "              'skip': [True, False], # skip\n",
    "              'batchnorm': [True, False]  # batchnorm\n",
    "}\n",
    "labels = [\"$\\epsilon$ sharpness\", \"Pac Bayes\", \"$||H||_{F}$\", \"FRN\", \"Classical Entropy\", \"Local Entropy\",\"Tr(H)\", \"Low pass filter\"]\n",
    "all_measures = [\"eps_flat\", \"pac_bayes\", \"fro_norm\", \"fim\", \"shannon_entropy\",\"local_entropy_grad_norm\", \"eig_trace\", \"low_pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(np.isnan(list({'a':1,'b':np.nan}.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_loss': 2.302539110183716, 'val_loss': 2.3025853633880615, 'train_acc': 10.001999855041504, 'val_acc': 10.0}\n",
      "{'train_loss': 0.011261606588959694, 'val_loss': 1.85854971408844, 'train_acc': 100.0, 'val_acc': 59.57999801635742}\n",
      "checkpoints/cifar10/resnet/895_0_0.5_6_0.0001_0.0075_512_False_False\n",
      "{'train_loss': 0.0771493911743164, 'val_loss': 1.5399298667907715, 'train_acc': 99.61199951171875, 'val_acc': 60.12999725341797}\n",
      "{'train_loss': 0.013769155368208885, 'val_loss': 2.0673892498016357, 'train_acc': 99.98400115966797, 'val_acc': 67.90999603271484}\n",
      "{'train_loss': 0.02191968262195587, 'val_loss': 2.1740357875823975, 'train_acc': 99.98600006103516, 'val_acc': 50.619998931884766}\n",
      "{'train_loss': 0.011698361486196518, 'val_loss': 1.7721710205078125, 'train_acc': 99.99800109863281, 'val_acc': 60.209999084472656}\n",
      "{'train_loss': 0.017319433391094208, 'val_loss': 1.9995100498199463, 'train_acc': 99.96599578857422, 'val_acc': 68.22000122070312}\n",
      "{'train_loss': 0.0821538195014, 'val_loss': 1.7476838827133179, 'train_acc': 98.58799743652344, 'val_acc': 67.0999984741211}\n",
      "{'train_loss': 0.02187964878976345, 'val_loss': 2.2688088417053223, 'train_acc': 99.99199676513672, 'val_acc': 50.48999786376953}\n",
      "{'train_loss': 0.0991269052028656, 'val_loss': 1.65240478515625, 'train_acc': 98.08999633789062, 'val_acc': 67.44999694824219}\n",
      "{'train_loss': 0.07838944345712662, 'val_loss': 1.7676515579223633, 'train_acc': 98.7179946899414, 'val_acc': 67.3499984741211}\n",
      "{'train_loss': 0.06954947859048843, 'val_loss': 1.5369151830673218, 'train_acc': 99.70399475097656, 'val_acc': 60.439998626708984}\n",
      "{'train_loss': 0.011139530688524246, 'val_loss': 1.8006484508514404, 'train_acc': 100.0, 'val_acc': 59.8599967956543}\n",
      "{'train_loss': 2.3025851249694824, 'val_loss': 2.3025858402252197, 'train_acc': 10.0, 'val_acc': 10.0}\n",
      "{'train_loss': 0.07141342759132385, 'val_loss': 1.5773448944091797, 'train_acc': 99.64199829101562, 'val_acc': 59.79999923706055}\n",
      "{'train_loss': 0.019903069362044334, 'val_loss': 2.254089117050171, 'train_acc': 99.98999786376953, 'val_acc': 49.94999694824219}\n",
      "checkpoints/cifar10/resnet/533_0_0.9_8_0.0_0.01_32_False_False\n",
      "{'train_loss': nan, 'val_loss': nan, 'train_acc': 10.0, 'val_acc': 10.0, 'shannon_entropy': nan, 'eps_flat': nan, 'pac_bayes': nan, 'fro_norm': nan, 'fim': nan}\n",
      "{'train_loss': 0.15112894773483276, 'val_loss': 2.7622718811035156, 'train_acc': 96.94200134277344, 'val_acc': 53.38999938964844}\n",
      "{'train_loss': 0.014821887947618961, 'val_loss': 2.036100387573242, 'train_acc': 99.97799682617188, 'val_acc': 68.1199951171875}\n",
      "{'train_loss': 8.515703201293945, 'val_loss': 8.51569938659668, 'train_acc': 10.0, 'val_acc': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# check problematic models\n",
    "inp = []\n",
    "for folder in glob.glob(\"checkpoints/cifar10/resnet/*_0_*\"):\n",
    "    try:\n",
    "        name = glob.glob(f\"{folder}/run_ms_0/measures.pkl\")[0]\n",
    "        with open(name, 'rb') as f:\n",
    "            measures = pickle.load(f)\n",
    "        if measures['train_loss'] > 0.01:\n",
    "            print(measures)\n",
    "        elif any(np.isnan(list(measures.values()))):\n",
    "            print(measures)\n",
    "    except:\n",
    "        print(folder)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file for ANNA\n",
    "idx = 0\n",
    "with open('results/csv_plots.csv', 'w') as f:\n",
    "    head = \"Experiment No,momentum,width,weight_decay,\" + \\\n",
    "           \"learning rate,batch_size,skip connection,batch normalization,\" + \\\n",
    "           \"train_loss,generalization gap,\" + \",\".join(labels) + '\\n'\n",
    "    f.write(head)\n",
    "\n",
    "# pick a hyper-parameter\n",
    "for i,grid in enumerate(ParameterGrid(param_grid)):\n",
    "    name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "           f\"{i}_0_{grid['mo']}_{grid['width']}_{grid['wd']}_\" \\\n",
    "           f\"{grid['lr']}_{grid['bs']}_{grid['skip']}_{grid['batchnorm']}\"\n",
    "    try:\n",
    "        with open(f\"{name}/run_ms_0/measures.pkl\", 'rb') as f:\n",
    "            measures = pickle.load(f)\n",
    "\n",
    "        inp = f\"{measures['train_loss']}, {-measures['val_acc']+measures['train_acc']},\"\n",
    "        for m in all_measures:\n",
    "            try:inp += f\"{measures[m]},\"\n",
    "            except:inp +=f'nan,'\n",
    "    except:\n",
    "        inp = 'nan,'\n",
    "        for m in all_measures:\n",
    "            inp += 'nan,' \n",
    "\n",
    "    with open('results/csv_plots.csv', 'a') as f:\n",
    "        inp = f\"{i},{grid['mo']},{grid['width']},{grid['wd']},{grid['lr']},{grid['bs']},{grid['skip']},{grid['batchnorm']},\" + inp + '\\n'\n",
    "        f.write(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plots for anna\n",
    "idx = 0\n",
    "# pick a measure\n",
    "for meas_idx, meas in enumerate(all_measures):\n",
    "    # pick a hyper-parameter\n",
    "    for key, value in param_grid.items():\n",
    "        fig,ax = plt.subplots()\n",
    "        grid = copy.deepcopy(param_grid)\n",
    "        del grid[key]\n",
    "        grid = list(ParameterGrid(grid))\n",
    "\n",
    "        for i,v in enumerate(value):\n",
    "            plotting_needs= [[], []]\n",
    "            # loop over all other set of hyper-parameters\n",
    "            for params in grid:\n",
    "                params = copy.deepcopy(params)\n",
    "                params[key] = v\n",
    "                name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "                       f\"*_0_{params['mo']}_{params['width']}_{params['wd']}_\" \\\n",
    "                       f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "\n",
    "                fol = glob.glob(name)[0]\n",
    "                try:\n",
    "                    with open(f\"{fol}/run_ms_0/measures.pkl\", 'rb') as f:\n",
    "                        measures = pickle.load(f)            \n",
    "                    if measures[\"train_loss\"] > 0.01:\n",
    "                        continue\n",
    "                    plotting_needs[0] += [measures[meas]]\n",
    "                except:\n",
    "                    continue\n",
    "                plotting_needs[1] += [-measures[\"val_acc\"]+measures[\"train_acc\"]]\n",
    "\n",
    "            ax.scatter(plotting_needs[0], plotting_needs[1], label=f\"{key}:{v}\")\n",
    "            ax.set_ylabel(\"Genralization Gap\",fontsize=16)\n",
    "            ax.set_xlabel(f\"{labels[meas_idx]}\",fontsize=16)\n",
    "            ax.set_title(f\"{labels[meas_idx]}\",fontsize=16)\n",
    "            ax.legend(fontsize=16)\n",
    "#             ax.set_ylim([0,0.05])\n",
    "#             fig.savefig(f\"results/{key}_{meas}.png\")\n",
    "#             plt.close()\n",
    "            plt.show()\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average the results over seed\n",
    "# same results as fantastic papers\n",
    "print(\"Measure\")\n",
    "for x in param_grid.keys():\n",
    "    print(x, end=', ')\n",
    "print(' ')\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "idx = 0\n",
    "# pick a measure\n",
    "for meas in all_measures:\n",
    "    print(f\"{labels[idx]} & \", end='')\n",
    "\n",
    "    # pick a hyper-parameter\n",
    "    for key, value in param_grid.items():\n",
    "\n",
    "        grid = copy.deepcopy(param_grid)\n",
    "        del grid[key]\n",
    "        \n",
    "        grid = list(ParameterGrid(grid))\n",
    "        mean_corr = []\n",
    "        for ms in [0, 1, 2]:\n",
    "            corr = []\n",
    "            # loop over all other set of hyper-parameters\n",
    "            for params in grid:\n",
    "                flat_measure = []\n",
    "                gen_gap = []\n",
    "                # and just vary a single hyper-parameter that we picked\n",
    "                for v in value:\n",
    "                    params[key] = v\n",
    "                    # cifar\n",
    "                    name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "                           f\"*_{ms}_{params['mo']}_{params['width']}_{params['wd']}_\" \\\n",
    "                           f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "                    fol = glob.glob(name)[0]\n",
    "                    try:\n",
    "                        with open(f\"{fol}/run_ms_{ms}/measures.pkl\", 'rb') as f:\n",
    "                            measures = pickle.load(f)\n",
    "                    except:\n",
    "                        continue\n",
    "                    # discard model with less cross-entropy               \n",
    "                    if measures[\"train_loss\"] > 0.01:\n",
    "                        continue\n",
    "                    # record flatness and gen_gap for it\n",
    "                    try:\n",
    "                        flat_measure.append(measures[meas])\n",
    "                    except:\n",
    "                        continue\n",
    "                    gen_gap.append(-measures[\"val_acc\"]+measures[\"train_acc\"])\n",
    "\n",
    "                # compute tau and append (this is inner tau in equation 4 of fantastic)\n",
    "                # just that our tau is not kendall but pearson\n",
    "                if len(gen_gap) > 1:\n",
    "                    c = scipy.stats.kendalltau(flat_measure, gen_gap)[0]\n",
    "                    if not math.isnan(c):\n",
    "                        corr.append(c)\n",
    "            mean_corr.append(np.mean(corr))\n",
    "        # this is mean over a picked hyper-parameter\n",
    "        print(f\"{np.mean(mean_corr):0.4f} & \", end='')\n",
    "    idx+=1\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure\n",
      "mo, width, wd, lr, bs, skip, batchnorm,  \n",
      "$\\epsilon$ sharpness & 0.8646 & 0.0548 & 0.3455 & 0.5464 & 0.9256 & -0.2291 & -0.0936 &  \n",
      "Pac Bayes & 0.9831 & -0.6960 & 0.4228 & 0.8472 & 0.9865 & 0.4133 & -0.0766 &  \n",
      "$||H||_{F}$ & 0.9563 & 0.0853 & 0.3602 & 0.6853 & 0.9632 & -0.0107 & -0.0851 &  \n",
      "FRN & 0.8098 & 0.2606 & 0.1614 & 0.4414 & 0.8198 & -0.0707 & -0.0936 &  \n",
      "Classical Entropy & 0.6756 & 0.1612 & -0.0018 & 0.2762 & 0.7151 & -0.2077 & -0.1106 &  \n",
      "Local Entropy & 0.2810 & -0.7381 & 0.2757 & 0.2207 & 0.0873 & 0.1906 & 0.0426 &  \n",
      "Tr(H) & 0.9629 & 0.1622 & 0.3545 & 0.7165 & 0.9701 & 0.2291 & -0.0809 &  \n",
      "Low pass filter & 0.9817 & -0.5387 & 0.4283 & 0.8138 & 0.9824 & 0.5717 & -0.0681 &  \n"
     ]
    }
   ],
   "source": [
    "# same results as fantastic papers\n",
    "print(\"Measure\")\n",
    "for x in param_grid.keys():\n",
    "    print(x, end=', ')\n",
    "print(' ')\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "idx = 0\n",
    "# pick a measure\n",
    "for meas in all_measures:\n",
    "    print(f\"{labels[idx]} & \", end='')\n",
    "\n",
    "    # pick a hyper-parameter\n",
    "    for key, value in param_grid.items():\n",
    "\n",
    "        grid = copy.deepcopy(param_grid)\n",
    "        del grid[key]\n",
    "        \n",
    "        grid = list(ParameterGrid(grid))\n",
    "        corr = []\n",
    "        # loop over all other set of hyper-parameters\n",
    "        for params in grid:\n",
    "            flat_measure = []\n",
    "            gen_gap = []\n",
    "            # and just vary a single hyper-parameter that we picked\n",
    "            for v in value:\n",
    "                params[key] = v\n",
    "                for ms in [0]:\n",
    "                    # cifar\n",
    "                    name = f\"checkpoints/check/resnet/\" \\\n",
    "                           f\"*_{ms}_{params['mo']}_{params['width']}_{params['wd']}_\" \\\n",
    "                           f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "\n",
    "                    fol = glob.glob(name)[0]\n",
    "\n",
    "                    try:\n",
    "                        with open(f\"{fol}/run_ms_{ms}/measures.pkl\", 'rb') as f:\n",
    "                            measures = pickle.load(f)\n",
    "                    except:\n",
    "                        continue\n",
    "                    # discard model with less cross-entropy               \n",
    "                    if measures[\"train_loss\"] > 0.01:\n",
    "                        continue\n",
    "\n",
    "                    # record flatness and gen_gap for it\n",
    "                    try:\n",
    "                        flat_measure.append(measures[meas])\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                    gen_gap.append(-measures[\"val_acc\"]+measures[\"train_acc\"])\n",
    "\n",
    "            # compute tau and append (this is inner tau in equation 4 of fantastic)\n",
    "            # just that our tau is not kendall but pearson\n",
    "            if len(gen_gap) > 1:\n",
    "                ###############################################################\n",
    "#                 YAHAN FULL CHUTIYAP HAI\n",
    "                #################################################################\n",
    "                if not any(np.isnan(flat_measure)):\n",
    "                    c = scipy.stats.pearsonr(np.log(flat_measure), gen_gap)[0]\n",
    "                    corr.append(c)\n",
    "        # this is mean over a picked hyper-parameter\n",
    "        print(f\"{np.mean(corr):0.4f} & \", end='')\n",
    "    idx+=1\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empirical order & \", end=' ')\n",
    "for key, value in param_grid.items():\n",
    "    print(f\"{key} &\", end=' ')\n",
    "print(' ')\n",
    "\n",
    "for key, value in param_grid.items():\n",
    "\n",
    "    grid = copy.deepcopy(param_grid)\n",
    "    del grid[key]\n",
    "\n",
    "    grid = list(ParameterGrid(grid))\n",
    "    corr = []\n",
    "    # loop over all other set of hyper-parameters\n",
    "    for params in grid:\n",
    "        gen_gap = []\n",
    "        hyp = []\n",
    "        # and just vary a single hyper-parameter that we picked\n",
    "        for v in value:\n",
    "            params[f\"{key}\"] = v\n",
    "            # cifar\n",
    "            name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "                   f\"*_0_{params['mo']}_{params['width']}_{params['wd']}_\" \\\n",
    "                   f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "\n",
    "            fol = glob.glob(name)[0]\n",
    "\n",
    "            try:\n",
    "                with open(f\"{fol}/run_ms_0/measures.pkl\", 'rb') as f:\n",
    "                    measures = pickle.load(f)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            if measures[\"train_loss\"] > 0.01:\n",
    "                continue\n",
    "            else:\n",
    "                # record flatness and hyper-parameter for it\n",
    "                gen_gap.append((100 - measures[\"val_acc\"]))\n",
    "                if v is True:\n",
    "                    hyp.append(1)\n",
    "                elif v is False:\n",
    "                    hyp.append(0)\n",
    "                else:\n",
    "                    hyp.append(v)\n",
    "\n",
    "        # compute tau and append (this is inner tau in equation 4 of fantastic)\n",
    "        # just that our tau is not kendall but pearson\n",
    "        if len(gen_gap) > 1:\n",
    "            c = scipy.stats.kendalltau(hyp, gen_gap)[0]\n",
    "            if not math.isnan(c):\n",
    "                corr.append(c)\n",
    "    # this is mean over a picked hyper-parameter\n",
    "    print(f\"{np.mean(corr):0.4f} & \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"measure, momentum, weight decay, learning rate, batch size\")\n",
    "idx = 0\n",
    "for meas in all_measures:\n",
    "    print(f\"{meas} & \", end='')\n",
    "    for key, value in param_grid.items():\n",
    "        grid = copy.deepcopy(param_grid)\n",
    "        del grid[key]\n",
    "        \n",
    "        grid = list(ParameterGrid(grid))\n",
    "        corr = []\n",
    "        for params in grid:\n",
    "            flat_measure = []\n",
    "            hyper_param = []\n",
    "            for v in value:\n",
    "                params[f\"{key}\"] = v\n",
    "                #mnist\n",
    "#                 name = f\"checkpoints/mnist/lenet/\" \\\n",
    "#                        f\"*_0_{params['mo']}_{params['wd']}\" \\\n",
    "#                        f\"_{params['lr']}_{params['bs']}_{False}\"\n",
    "                # cifar\n",
    "#                 name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "#                        f\"*_0_{params['mo']}_{params['width']}_{params['wd']}_\" \\\n",
    "#                        f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "\n",
    "                fol = glob.glob(name)[0]\n",
    "\n",
    "                with open(f\"{fol}/run_ms_0/measures.pkl\", 'rb') as f:\n",
    "                    measures = pickle.load(f)\n",
    "\n",
    "                if np.nan in list(measures.values()):\n",
    "                    continue\n",
    "\n",
    "                if measures['train_loss'] > 0.01:\n",
    "                    continue\n",
    "                else:\n",
    "                    flat_measure.append(measures[meas])\n",
    "                    if v is True:\n",
    "                        hyper_param.append(1)\n",
    "                    elif v is False:\n",
    "                        hyper_param.append(0)\n",
    "                    else:\n",
    "                        hyper_param.append(v)\n",
    "            if len(hyper_param)>1:\n",
    "                corr.append(scipy.stats.pearsonr(hyper_param, flat_measure)[0])\n",
    "\n",
    "        print(f\"{np.mean(corr):0.3f} & \", end='')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plots for anna\n",
    "idx = 0\n",
    "# pick a measure\n",
    "for meas_idx, meas in enumerate(all_measures):\n",
    "    fig,ax = plt.subplots()\n",
    "    for idx, width in enumerate([4, 6, 8]):\n",
    "        plotting_needs = [[], []]\n",
    "        for params in list(ParameterGrid(param_grid)):\n",
    "            name = f\"checkpoints/cifar10/resnet/\" \\\n",
    "                   f\"*_0_{params['mo']}_{width}_{params['wd']}_\" \\\n",
    "                   f\"{params['lr']}_{params['bs']}_{params['skip']}_{params['batchnorm']}\"\n",
    "\n",
    "            fol = glob.glob(name)[0]\n",
    "            try:\n",
    "                with open(f\"{fol}/run_ms_0/measures.pkl\", 'rb') as f:\n",
    "                    measures = pickle.load(f)            \n",
    "                if measures[\"train_loss\"] > 0.01:\n",
    "                    continue\n",
    "                if not np.isnan(measures[meas]):\n",
    "                    plotting_needs[0] += [measures[meas]]\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            plotting_needs[1] += [-measures[\"val_acc\"]+measures[\"train_acc\"]]\n",
    "        ax.scatter([x / np.max(plotting_needs[0]) for x in plotting_needs[0]], plotting_needs[1], label = f\"Width={width}\")\n",
    "    ax.set_ylabel(\"Genralization Gap\",fontsize=16)\n",
    "    ax.set_xlabel(f\"{labels[meas_idx]}\",fontsize=16)\n",
    "    ax.set_xscale('log')\n",
    "    ax.legend(fontsize=16)\n",
    "    fig.savefig(f\"results/scatter/{labels[meas_idx]}.png\")\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
