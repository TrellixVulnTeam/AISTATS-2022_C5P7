I0121 13:39:59.506605 23381658941248 train.py:126] {'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': 0, 'verbosity': 0, 'logger_levels': {}, 'stderrthreshold': 'fatal', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'pdb': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'op_conversion_fallback_to_while_loop': True, 'runtime_oom_exit': True, 'hbm_oom_exit': True, 'test_random_seed': 301, 'test_srcdir': '', 'test_tmpdir': '/state/partition1/job-1398236/absl_testing', 'test_randomize_ordering_seed': '', 'xml_output_file': '', 'cutout_length': 16, 'wikipedia_auto_select_flume_mode': True, 'use_test_set': True, 'randaug_num_layers': 2, 'randaug_magnitude': 9, 'imagenet_mixup_alpha': 0.0, 'from_pretrained_checkpoint': False, 'efficientnet_checkpoint_path': None, 'use_additional_skip_connections': False, 'gradient_clipping': 5.0, 'learning_rate': 0.1, 'use_learning_rate_schedule': True, 'use_std_schedule': True, 'weight_decay': 0.001, 'run_seed': 0, 'use_rmsprop': False, 'lr_schedule': 'cosine', 'std_schedule': 'cosine', 'save_progress_seconds': 3600, 'additional_checkpoints_at_epochs': [], 'also_eval_on_training_set': False, 'compute_top_5_error_rate': False, 'label_smoothing': 0.0, 'ema_decay': 0.0, 'no_weight_decay_on_bn': False, 'evaluate_every': 1, 'ssgd_std': 0.0001, 'sam_rho': -1.0, 'sync_perturbations': False, 'inner_group_size': None, 'dataset': 'cifar10', 'model_name': 'WideResnet28x10', 'num_epochs': 200, 'batch_size': 128, 'output_dir': 'checkpoints/', 'image_level_augmentations': 'basic', 'batch_level_augmentations': 'none', '?': False, 'help': False, 'helpshort': False, 'helpfull': False, 'helpxml': False}
I0121 13:41:17.288653 23381658941248 train.py:141] Total batch size: 128 (128 x 1 replicas)
I0121 13:41:17.290084 23381658941248 dataset_info.py:354] Load dataset info from tf_data/cifar10/3.0.2
I0121 13:41:17.293274 23381658941248 dataset_builder.py:354] Reusing dataset cifar10 (tf_data/cifar10/3.0.2)
I0121 13:41:17.293470 23381658941248 dataset_builder.py:569] Constructing tf.data.Dataset for split train[0:50000], from tf_data/cifar10/3.0.2
I0121 13:41:17.344439 23381658941248 dataset_info.py:354] Load dataset info from tf_data/cifar10/3.0.2
I0121 13:41:17.345397 23381658941248 dataset_builder.py:354] Reusing dataset cifar10 (tf_data/cifar10/3.0.2)
I0121 13:41:17.345476 23381658941248 dataset_builder.py:569] Constructing tf.data.Dataset for split test, from tf_data/cifar10/3.0.2
I0121 13:41:17.361752 23381658941248 dataset_source.py:191] Used test set instead of validation set.
I0121 13:41:28.223717 23381658941248 flax_training.py:866] Starting training from scratch.
I0121 13:44:00.742116 23381658941248 flax_training.py:810] Whole training step done in 150.84214568138123 (390 steps)
I0121 13:44:00.749576 23381658941248 flax_training.py:923] Epoch 0 finished in 150.85s.
I0121 13:44:00.749697 23381658941248 flax_training.py:928] Evaluating at end of epoch 0 (0-indexed)
I0121 13:44:10.657109 23381658941248 flax_training.py:962] Evaluated model in 9.91.
I0121 13:46:11.526862 23381658941248 flax_training.py:810] Whole training step done in 120.86938309669495 (390 steps)
I0121 13:46:11.530791 23381658941248 flax_training.py:923] Epoch 1 finished in 120.87s.
I0121 13:46:11.530924 23381658941248 flax_training.py:928] Evaluating at end of epoch 1 (0-indexed)
I0121 13:46:18.166880 23381658941248 flax_training.py:962] Evaluated model in 6.64.
I0121 13:48:42.833669 23381658941248 flax_training.py:810] Whole training step done in 144.66650128364563 (390 steps)
I0121 13:48:42.837416 23381658941248 flax_training.py:923] Epoch 2 finished in 144.67s.
I0121 13:48:42.837537 23381658941248 flax_training.py:928] Evaluating at end of epoch 2 (0-indexed)
I0121 13:48:49.477508 23381658941248 flax_training.py:962] Evaluated model in 6.64.
I0121 13:50:50.264465 23381658941248 flax_training.py:810] Whole training step done in 120.7866563796997 (390 steps)
I0121 13:50:50.272967 23381658941248 flax_training.py:923] Epoch 3 finished in 120.80s.
I0121 13:50:50.273082 23381658941248 flax_training.py:928] Evaluating at end of epoch 3 (0-indexed)
I0121 13:50:56.906220 23381658941248 flax_training.py:962] Evaluated model in 6.63.
I0121 13:55:41.193728 23381658941248 flax_training.py:810] Whole training step done in 284.2872107028961 (390 steps)
I0121 13:55:41.199836 23381658941248 flax_training.py:923] Epoch 4 finished in 284.29s.
I0121 13:55:41.199956 23381658941248 flax_training.py:928] Evaluating at end of epoch 4 (0-indexed)
I0121 13:55:47.797229 23381658941248 flax_training.py:962] Evaluated model in 6.60.
